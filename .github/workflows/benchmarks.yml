name: Benchmarks

on:
  push:
    branches: [ main, phase-* ]
    paths:
      - 'benchmarks/**'
      - 'sources/**'
      - '.github/workflows/benchmarks.yml'
  pull_request:
    branches: [ main ]
    paths:
      - 'benchmarks/**'
      - 'sources/**'
  workflow_dispatch:
    inputs:
      save_baseline:
        description: 'Save as baseline'
        required: false
        default: 'false'

jobs:
  benchmark:
    name: Run Performance Benchmarks
    runs-on: ${{ matrix.os }}
    timeout-minutes: 30

    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-22.04, macos-13]
        compiler: [clang]
        build_type: [Release]

    steps:
    - name: Checkout monitoring_system
      uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Full history for comparison
        path: monitoring_system

    - name: Checkout common_system
      uses: actions/checkout@v4
      with:
        repository: kcenon/common_system
        path: common_system
        token: ${{ secrets.GITHUB_TOKEN }}

    - name: Build and install common_system (Linux)
      if: runner.os == 'Linux'
      run: |
        cd common_system
        cmake -B build -G Ninja \
          -DCMAKE_BUILD_TYPE=Release \
          -DCMAKE_INSTALL_PREFIX="${GITHUB_WORKSPACE}/common_system_install"
        cmake --build build --parallel
        cmake --install build
        cd ..

    - name: Build and install common_system (macOS)
      if: runner.os == 'macOS'
      run: |
        cd common_system
        cmake -B build -G Ninja \
          -DCMAKE_BUILD_TYPE=Release \
          -DCMAKE_INSTALL_PREFIX="${GITHUB_WORKSPACE}/common_system_install"
        cmake --build build --parallel
        cmake --install build
        cd ..

    - name: Install dependencies (Ubuntu)
      if: runner.os == 'Linux'
      run: |
        sudo apt-get update
        sudo apt-get install -y cmake ninja-build clang libbenchmark-dev libgtest-dev libfmt-dev

    - name: Install dependencies (macOS)
      if: runner.os == 'macOS'
      run: |
        brew install ninja google-benchmark googletest fmt

    - name: Set up compiler
      run: |
        echo "CC=clang" >> $GITHUB_ENV
        echo "CXX=clang++" >> $GITHUB_ENV

    - name: Configure CMake
      run: |
        cd monitoring_system
        cmake -B build -S . \
          -GNinja \
          -DCMAKE_BUILD_TYPE=${{ matrix.build_type }} \
          -DCMAKE_PREFIX_PATH="${GITHUB_WORKSPACE}/common_system_install" \
          -DMONITORING_BUILD_BENCHMARKS=ON \
          -DBUILD_TESTS=OFF \
          -DMONITORING_STANDALONE=ON

    - name: Build benchmarks
      run: |
        cd monitoring_system
        cmake --build build --config ${{ matrix.build_type }} --target monitoring_benchmarks -j

    - name: Run benchmarks
      run: |
        cd monitoring_system/build/benchmarks
        ./monitoring_benchmarks \
          --benchmark_format=json \
          --benchmark_out=benchmark_results_${{ matrix.os }}.json \
          --benchmark_repetitions=3 \
          --benchmark_report_aggregates_only=true

    - name: Run benchmarks (console output)
      run: |
        cd monitoring_system/build/benchmarks
        ./monitoring_benchmarks \
          --benchmark_filter="Profiler" \
          --benchmark_repetitions=3

    - name: Upload benchmark results
      uses: actions/upload-artifact@v4
      with:
        name: benchmark-results-${{ matrix.os }}
        path: monitoring_system/build/benchmarks/benchmark_results_${{ matrix.os }}.json
        retention-days: 30

    - name: Save baseline (if requested)
      if: github.event.inputs.save_baseline == 'true' && github.ref == 'refs/heads/main'
      run: |
        cd monitoring_system
        mkdir -p benchmarks/baselines
        cp build/benchmarks/benchmark_results_${{ matrix.os }}.json \
           benchmarks/baselines/baseline_${{ matrix.os }}_$(date +%Y%m%d).json

    - name: Compare with baseline (if exists)
      if: hashFiles('monitoring_system/benchmarks/baselines/baseline_${{ matrix.os }}_*.json') != ''
      run: |
        cd monitoring_system
        echo "Comparing with baseline..."
        # Install compare.py from Google Benchmark tools
        pip3 install --user scipy

        # Find latest baseline
        BASELINE=$(ls -t benchmarks/baselines/baseline_${{ matrix.os }}_*.json | head -1)

        if [ -f "$BASELINE" ]; then
          echo "Baseline: $BASELINE"
          echo "Current: build/benchmarks/benchmark_results_${{ matrix.os }}.json"

          # Download compare.py
          curl -O https://raw.githubusercontent.com/google/benchmark/main/tools/compare.py
          chmod +x compare.py

          # Run comparison
          python3 compare.py \
            "$BASELINE" \
            build/benchmarks/benchmark_results_${{ matrix.os }}.json
        fi

    - name: Check for performance regression
      run: |
        cd monitoring_system
        echo "# Performance Regression Analysis" | tee -a $GITHUB_STEP_SUMMARY
        echo "" | tee -a $GITHUB_STEP_SUMMARY

        # Extract key metrics from benchmark results
        RESULTS="build/benchmarks/benchmark_results_${{ matrix.os }}.json"

        if [ -f "$RESULTS" ]; then
          # Define regression thresholds (based on docs/PERFORMANCE_BASELINE.md)
          # Recording (single): 292 ns baseline, 350 ns max (20% regression)
          # Recording (concurrent): 1094 ns baseline, 1200 ns max (10% regression)
          # Scoped timer: 326 ns baseline, 400 ns max (23% regression)

          # Extract mean values using jq
          RECORDING_SINGLE=$(jq -r '.benchmarks[] | select(.name == "BM_ProfilerRecording_Single_mean") | .cpu_time' "$RESULTS" || echo "0")
          RECORDING_CONCURRENT=$(jq -r '.benchmarks[] | select(.name == "BM_ProfilerRecording_Concurrent/threads:4_mean") | .cpu_time' "$RESULTS" || echo "0")
          SCOPED_TIMER=$(jq -r '.benchmarks[] | select(.name == "BM_ScopedTimer_Overhead_mean") | .cpu_time' "$RESULTS" || echo "0")

          echo "## Measured Performance (${{ matrix.os }})" | tee -a $GITHUB_STEP_SUMMARY
          echo "" | tee -a $GITHUB_STEP_SUMMARY
          echo "| Metric | Current | Baseline | Threshold | Status |" | tee -a $GITHUB_STEP_SUMMARY
          echo "|--------|---------|----------|-----------|--------|" | tee -a $GITHUB_STEP_SUMMARY

          REGRESSION_DETECTED=0

          # Check Recording Single
          if [ "$RECORDING_SINGLE" != "0" ]; then
            echo -n "| Recording (single) | ${RECORDING_SINGLE} ns | 292 ns | 350 ns | " | tee -a $GITHUB_STEP_SUMMARY
            if (( $(echo "$RECORDING_SINGLE > 350" | bc -l) )); then
              echo "❌ **REGRESSION**" | tee -a $GITHUB_STEP_SUMMARY
              REGRESSION_DETECTED=1
            else
              echo "✅ Pass |" | tee -a $GITHUB_STEP_SUMMARY
            fi
          fi

          # Check Recording Concurrent
          if [ "$RECORDING_CONCURRENT" != "0" ]; then
            echo -n "| Recording (concurrent) | ${RECORDING_CONCURRENT} ns | 921 ns | 1200 ns | " | tee -a $GITHUB_STEP_SUMMARY
            if (( $(echo "$RECORDING_CONCURRENT > 1200" | bc -l) )); then
              echo "❌ **REGRESSION** |" | tee -a $GITHUB_STEP_SUMMARY
              REGRESSION_DETECTED=1
            else
              echo "✅ Pass |" | tee -a $GITHUB_STEP_SUMMARY
            fi
          fi

          # Check Scoped Timer
          if [ "$SCOPED_TIMER" != "0" ]; then
            echo -n "| Scoped timer | ${SCOPED_TIMER} ns | 326 ns | 400 ns | " | tee -a $GITHUB_STEP_SUMMARY
            if (( $(echo "$SCOPED_TIMER > 400" | bc -l) )); then
              echo "❌ **REGRESSION** |" | tee -a $GITHUB_STEP_SUMMARY
              REGRESSION_DETECTED=1
            else
              echo "✅ Pass |" | tee -a $GITHUB_STEP_SUMMARY
            fi
          fi

          echo "" | tee -a $GITHUB_STEP_SUMMARY

          if [ $REGRESSION_DETECTED -eq 1 ]; then
            echo "## ⚠️ Performance Regression Detected!" | tee -a $GITHUB_STEP_SUMMARY
            echo "" | tee -a $GITHUB_STEP_SUMMARY
            echo "One or more benchmarks exceeded regression thresholds." | tee -a $GITHUB_STEP_SUMMARY
            echo "Review changes and optimize before merging." | tee -a $GITHUB_STEP_SUMMARY
            exit 1
          else
            echo "## ✅ No Performance Regression Detected" | tee -a $GITHUB_STEP_SUMMARY
            echo "" | tee -a $GITHUB_STEP_SUMMARY
            echo "All benchmarks within acceptable thresholds." | tee -a $GITHUB_STEP_SUMMARY
          fi
        else
          echo "❌ Benchmark results file not found" | tee -a $GITHUB_STEP_SUMMARY
          exit 1
        fi

  report:
    name: Generate Benchmark Report
    needs: benchmark
    runs-on: ubuntu-22.04
    if: always()

    steps:
    - name: Download all artifacts
      uses: actions/download-artifact@v4

    - name: Generate summary
      run: |
        echo "# Benchmark Results Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "## Phase 0: Baseline Measurement" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY

        for dir in benchmark-results-*; do
          if [ -d "$dir" ]; then
            echo "### $dir" >> $GITHUB_STEP_SUMMARY
            if [ -f "$dir/benchmark_results_"*.json ]; then
              echo "✅ Benchmarks completed" >> $GITHUB_STEP_SUMMARY
              echo "" >> $GITHUB_STEP_SUMMARY
            else
              echo "❌ No results found" >> $GITHUB_STEP_SUMMARY
              echo "" >> $GITHUB_STEP_SUMMARY
            fi
          fi
        done

        echo "## Next Steps" >> $GITHUB_STEP_SUMMARY
        echo "- Review benchmark results in artifacts" >> $GITHUB_STEP_SUMMARY
        echo "- Baseline documented in docs/PERFORMANCE_BASELINE.md" >> $GITHUB_STEP_SUMMARY
        echo "- Performance regression detection active" >> $GITHUB_STEP_SUMMARY
        echo "- Sprint 2 optimization targets defined" >> $GITHUB_STEP_SUMMARY
